eval_py_env = AssetTradingEnv()
eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)

loaded_policy = tf.compat.v2.saved_model.load('/content/model_save/policy_step_96_gamma.mdl') # CHANGE FOR SHORTER PERIODS
eval_policy = loaded_policy
CONFIG_EPISODE_LENGTH = 747
CONFIG_FILE = "test_data.csv"
CONFIG_NUM_EVAL_EPISODES = 1
all_returns = []
all_weights = []

for _ in range(CONFIG_NUM_EVAL_EPISODES):
    time_step = eval_env.reset()
    episode_returns = []
    episode_weights = []

    while not time_step.is_last():
        action_step = eval_policy.action(time_step)
        time_step = eval_env.step(action_step.action)

        # Append the return at each time step
        episode_returns.append(time_step.reward.numpy())
        weights = tf.nn.softmax(action_step.action.numpy()[0])
        episode_weights.append(weights)

    # Append the returns for the current episode
    all_returns.append(episode_returns)
    all_weights.append(episode_weights)

average_returns = np.mean(all_returns)
print(average_returns)
episode_returns = [arr * 100 for arr in episode_returns]
